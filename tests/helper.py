"""Helper functions for the tests."""


import argparse


def get_default_args() -> argparse.Namespace:
    """Get default arguments for the tests."""
    args = {
    "adapt_blocks": [],
    "alpha": 0.5,
    "auto_augment": [],
    "base_class": 60,
    "base_mode": "ft_cos",
    "batch_size_base": 64,
    "batch_size_new": 0,
    "beta": 0.5,
    "ce_loss_factor": 1.0,
    "constrained_cropping": False,
    "dataroot": "./data",
    "dataset": "cifar100",
    "debug": False,
    "decay": 0.0005,
    "distributed": False,
    "distributed_backend": "nccl",
    "distributed_launcher": "pytorch",
    "encoder": "vit-16",
    "encoder_lr_factor": 0.1,
    "epochs_base": 100,
    "epochs_new": 10,
    "eval_freq": 15,
    "exp_name": "cub200_baseline",
    "encoder_ft_start_layer": -1,
    "freeze_vit": True,
    "gamma": 0.1,
    "gpu": "0",
    "incft": False,
    "limited_base_class": -1,
    "limited_base_samples": 1,
    "lr_base": 0.1,
    "lr_new": 0.1,
    "lrb": 0.1,
    "lrw": 0.1,
    "max_scale_crops": [1, 0.14],
    "milestones": [60, 80, 100],
    "min_scale_crops": [0.2, 0.05],
    "moco_dim": 128,
    "moco_k": 65536,
    "moco_loss_factor": 0.1,
    "moco_m": 0.999,
    "moco_t": 0.07,
    "model_dir": None,
    "momentum": 0.9,
    "new_mode": "avg_cos",
    "not_data_init": False,
    "num_classes": 200,
    "num_crops": [2, 1],
    "num_mlp": 2,
    "num_workers": 0,
    "pet_cls": None,
    "pre_train_epochs": 0,
    "pre_train_lr": 0.001,
    "pre_trained_url": None,
    "rank": 5,
    "save_path": "checkpoint/cub200_baseline",
    "schedule": "Step",
    "seed": 1,
    "sessions": 11,
    "shot": 5,
    "start_session": 0,
    "step": 40,
    "temperature": 16,
    "test_batch_size": 100,
    "encoder_ft_start_epoch": 0,
    "way": 5,
    "add_bias_in_classifier": False,
    "pet_on_teacher": False,}

    return argparse.Namespace(**args)



def get_lora_args() -> argparse.Namespace:
    """Get default arguments for the tests."""
    args = {
    "adapt_blocks": [],
    "alpha": 0.5,
    "auto_augment": [],
    "base_class": 60,
    "base_mode": "ft_cos",
    "batch_size_base": 64,
    "batch_size_new": 0,
    "beta": 0.5,
    "ce_loss_factor": 1.0,
    "constrained_cropping": False,
    "dataroot": "./data",
    "dataset": "cifar100",
    "debug": False,
    "decay": 0.0005,
    "distributed": False,
    "distributed_backend": "nccl",
    "distributed_launcher": "pytorch",
    "encoder": "vit-16",
    "encoder_lr_factor": 0.1,
    "epochs_base": 100,
    "epochs_new": 10,
    "eval_freq": 15,
    "exp_name": "cub200_baseline",
    "encoder_ft_start_layer": -1,
    "freeze_vit": True,
    "gamma": 0.1,
    "gpu": "0",
    "incft": False,
    "limited_base_class": -1,
    "limited_base_samples": 1,
    "lr_base": 0.1,
    "lr_new": 0.1,
    "lrb": 0.1,
    "lrw": 0.1,
    "max_scale_crops": [1, 0.14],
    "milestones": [60, 80, 100],
    "min_scale_crops": [0.2, 0.05],
    "moco_dim": 128,
    "moco_k": 65536,
    "moco_loss_factor": 0.1,
    "moco_m": 0.999,
    "moco_t": 0.07,
    "model_dir": None,
    "momentum": 0.9,
    "new_mode": "avg_cos",
    "not_data_init": False,
    "num_classes": 200,
    "num_crops": [2, 1],
    "num_mlp": 2,
    "num_workers": 0,
    "pet_cls": "LoRA",
    "pre_train_epochs": 0,
    "pre_train_lr": 0.001,
    "pre_trained_url": None,
    "rank": 5,
    "save_path": "checkpoint/cub200_baseline",
    "schedule": "Step",
    "seed": 1,
    "sessions": 11,
    "shot": 5,
    "size_crops": [224, 96],
    "start_session": 0,
    "step": 40,
    "temperature": 16,
    "test_batch_size": 100,
    "encoder_ft_start_epoch": 0,
    "way": 5,
    "add_bias_in_classifier": False,
    "pet_on_teacher": False,}

    return argparse.Namespace(**args)
