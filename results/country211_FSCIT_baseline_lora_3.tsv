base_last	incremental_last	all_last	all_std_last	base	incremental	all	all_std	exp_name	dataset	dataroot	result_key	batch_size_base	epochs_base	lr_base	schedule	milestones	step	decay	momentum	ce_loss_factor	moco_dim	moco_k	moco_m	moco_t	moco_loss_factor	num_views	hf_model_checkpoint	encoder	num_mlp	shot	fsl_setup	incft	test_batch_size	eval_freq	num_workers	seed	num_seeds	pet_cls	rank	pet_tuning_start_epoch	encoder_ft_start_layer	adapt_blocks	encoder_ft_start_epoch	encoder_lr_factor	limited_base_class	limited_base_samples	pet_on_teacher	update_base_classifier_with_prototypes	start_training_with_prototypes	add_bias_in_classifier	base_class	way	num_classes	sessions	save_path	size_crops
6.97	7.51	7.45	0.01	[24.11, 18.42, 16.12, 13.33, 10.48, 9.32, 8.39, 7.99, 7.59, 6.97]	[nan, 16.79, 14.02, 11.21, 10.85, 9.66, 8.92, 8.12, 7.81, 7.51]	[24.11, 17.63, 14.75, 11.76, 10.77, 9.6, 8.84, 8.1, 7.79, 7.45]	[0.05, 0.04, 0.03, 0.02, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]	country211_baseline_lora_fscit	country211	./data	baseline_lora	64	10	0.01	Cosine	[60, 70]	40	0.0005	0.9	1.0	128	65536	0.999	0.07	0.0	2	google/vit-base-patch16-224-in21k	vit-b16	1	10	FSCIT	False	100	15	8	2	3	LoRA	5	0	100	[0, 1, 2]	0	1	-1	1.0	False	False	True	False	22	21	211	10	checkpoint/country211_baseline_lora_fscit	[224, 224]
7.58	7.37	7.39	0.0	[22.76, 18.03, 15.51, 13.39, 11.17, 10.08, 9.32, 8.92, 8.26, 7.58]	[nan, 15.98, 13.19, 10.72, 10.58, 9.41, 8.68, 7.92, 7.65, 7.37]	[22.76, 17.03, 13.99, 11.41, 10.7, 9.53, 8.78, 8.05, 7.72, 7.39]	[0.08, 0.03, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0]	country211_baseline_lora_fscit	country211	./data	baseline_lora	64	10	0.001	Cosine	[60, 70]	40	0.0005	0.9	1.0	128	65536	0.999	0.07	0.0	2	google/vit-base-patch16-224-in21k	vit-b16	1	10	FSCIT	False	100	15	8	2	3	LoRA	5	0	100	[0, 1, 2]	0	1	-1	1.0	False	False	True	False	22	21	211	10	checkpoint/country211_baseline_lora_fscit	[224, 224]
6.12	7.55	7.4	0.04	[24.32, 17.99, 15.38, 12.54, 9.56, 8.3, 7.56, 7.18, 6.68, 6.12]	[nan, 17.41, 14.4, 11.49, 11.0, 9.81, 9.03, 8.21, 7.88, 7.55]	[24.32, 17.7, 14.74, 11.76, 10.7, 9.54, 8.81, 8.07, 7.74, 7.4]	[0.07, 0.08, 0.07, 0.05, 0.06, 0.03, 0.03, 0.03, 0.05, 0.04]	country211_baseline_lora_fscit	country211	./data	baseline_lora	64	10	0.1	Cosine	[60, 70]	40	0.0005	0.9	1.0	128	65536	0.999	0.07	0.0	2	google/vit-base-patch16-224-in21k	vit-b16	1	10	FSCIT	False	100	15	8	2	3	LoRA	5	0	100	[0, 1, 2]	0	1	-1	1.0	False	False	True	False	22	21	211	10	checkpoint/country211_baseline_lora_fscit	[224, 224]
