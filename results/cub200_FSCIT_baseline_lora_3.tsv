base_last	incremental_last	all_last	all_std_last	base	incremental	all	all_std	exp_name	dataset	dataroot	result_key	batch_size_base	epochs_base	lr_base	schedule	milestones	step	decay	momentum	ce_loss_factor	moco_dim	moco_k	moco_m	moco_t	moco_loss_factor	num_views	hf_model_checkpoint	encoder	num_mlp	shot	fsl_setup	incft	test_batch_size	eval_freq	num_workers	seed	num_seeds	pet_cls	rank	pet_tuning_start_epoch	encoder_ft_start_layer	adapt_blocks	encoder_ft_start_epoch	encoder_lr_factor	limited_base_class	limited_base_samples	pet_on_teacher	update_base_classifier_with_prototypes	start_training_with_prototypes	add_bias_in_classifier	base_class	way	num_classes	sessions	save_path	size_crops
82.2	73.38	74.18	0.02	[86.22, 85.24, 83.75, 83.56, 83.37, 83.17, 82.39, 82.39, 82.2, 82.2]	[nan, 72.87, 78.6, 75.32, 77.71, 77.75, 75.64, 73.88, 72.17, 73.38]	[88.16, 78.56, 80.19, 77.39, 78.93, 78.92, 76.86, 75.03, 73.63, 74.18]	[0.24, 0.13, 0.07, 0.05, 0.05, 0.03, 0.03, 0.03, 0.02, 0.02]	cub200_baseline_lora_fscit	cub200	data/CUB_200_2011	baseline_lora	64	10	0.001	Cosine	[60, 70]	40	0.0005	0.9	1.0	128	65536	0.999	0.07	0.0	2	google/vit-base-patch16-224-in21k	vit-b16	1	10	FSCIT	False	100	15	8	2	3	LoRA	5	0	100	[0, 1, 2]	0	1	-1	1.0	False	False	True	False	20	20	200	10	checkpoint/cub200_baseline_lora_fscit	[224, 224]
84.79	73.36	74.39	0.0	[88.41, 87.51, 86.15, 85.95, 85.76, 85.57, 84.98, 84.98, 84.79, 84.79]	[nan, 72.35, 78.42, 75.2, 77.61, 77.67, 75.6, 73.85, 72.13, 73.36]	[90.06, 79.34, 80.8, 77.85, 79.28, 79.21, 77.15, 75.28, 73.86, 74.39]	[0.28, 0.16, 0.1, 0.07, 0.06, 0.05, 0.02, 0.01, 0.01, 0.0]	cub200_baseline_lora_fscit	cub200	data/CUB_200_2011	baseline_lora	64	10	0.01	Cosine	[60, 70]	40	0.0005	0.9	1.0	128	65536	0.999	0.07	0.0	2	google/vit-base-patch16-224-in21k	vit-b16	1	10	FSCIT	False	100	15	8	2	3	LoRA	5	0	100	[0, 1, 2]	0	1	-1	1.0	False	False	True	False	20	20	200	10	checkpoint/cub200_baseline_lora_fscit	[224, 224]
86.67	73.45	74.65	0.1	[90.49, 89.64, 88.09, 87.64, 87.51, 87.31, 86.73, 86.73, 86.67, 86.67]	[nan, 72.87, 78.65, 75.24, 77.62, 77.77, 75.71, 73.97, 72.27, 73.45]	[91.83, 80.62, 81.54, 78.25, 79.6, 79.55, 77.46, 75.58, 74.16, 74.65]	[0.13, 0.22, 0.1, 0.15, 0.12, 0.03, 0.11, 0.1, 0.13, 0.1]	cub200_baseline_lora_fscit	cub200	data/CUB_200_2011	baseline_lora	64	10	0.1	Cosine	[60, 70]	40	0.0005	0.9	1.0	128	65536	0.999	0.07	0.0	2	google/vit-base-patch16-224-in21k	vit-b16	1	10	FSCIT	False	100	15	8	2	3	LoRA	5	0	100	[0, 1, 2]	0	1	-1	1.0	False	False	True	False	20	20	200	10	checkpoint/cub200_baseline_lora_fscit	[224, 224]
