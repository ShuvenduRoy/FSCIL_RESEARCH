base_last	incremental_last	all_last	all_std_last	base	incremental	all	all_std	exp_name	dataset	dataroot	result_key	batch_size_base	epochs_base	lr_base	schedule	milestones	step	decay	momentum	ce_loss_factor	moco_dim	moco_k	moco_m	moco_t	moco_loss_factor	num_views	hf_model_checkpoint	encoder	num_mlp	shot	fsl_setup	incft	test_batch_size	eval_freq	num_workers	seed	num_seeds	pet_cls	rank	pet_tuning_start_epoch	encoder_ft_start_layer	adapt_blocks	encoder_ft_start_epoch	encoder_lr_factor	limited_base_class	limited_base_samples	pet_on_teacher	update_base_classifier_with_prototypes	start_training_with_prototypes	add_bias_in_classifier	base_class	way	num_classes	sessions	save_path	size_crops
47.0	55.64	54.69	0.02	[86.0, 75.0, 67.5, 65.5, 58.0, 54.0, 51.5, 51.0, 47.0]	[nan, 79.0, 77.25, 69.67, 66.75, 63.13, 59.7, 57.81, 55.64]	[86.0, 77.0, 74.0, 68.62, 65.0, 61.61, 58.52, 56.96, 54.69]	[0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.03, 0.03, 0.02]	dtd_baseline_lora_fscit	dtd	./data	baseline_lora	64	10	0.001	Cosine	[60, 70]	40	0.0005	0.9	1.0	128	65536	0.999	0.07	0.0	2	google/vit-base-patch16-224-in21k	vit-b16	1	10	FSCIT	False	100	15	8	2	3	LoRA	5	0	100	[0, 1, 2]	0	1	-1	1.0	False	False	True	False	5	5	47	9	checkpoint/dtd_baseline_lora_fscit	[224, 224]
46.5	55.67	54.65	0.05	[88.17, 78.0, 68.83, 67.17, 59.33, 54.0, 51.0, 50.67, 46.5]	[nan, 78.17, 77.33, 69.5, 66.87, 63.33, 59.83, 57.79, 55.67]	[88.17, 78.08, 74.5, 68.92, 65.37, 61.78, 58.57, 56.89, 54.65]	[0.24, 0.62, 0.14, 0.12, 0.12, 0.04, 0.06, 0.08, 0.05]	dtd_baseline_lora_fscit	dtd	./data	baseline_lora	64	10	0.1	Cosine	[60, 70]	40	0.0005	0.9	1.0	128	65536	0.999	0.07	0.0	2	google/vit-base-patch16-224-in21k	vit-b16	1	10	FSCIT	False	100	15	8	2	3	LoRA	5	0	100	[0, 1, 2]	0	1	-1	1.0	False	False	True	False	5	5	47	9	checkpoint/dtd_baseline_lora_fscit	[224, 224]
47.17	55.64	54.7	0.02	[86.17, 75.5, 67.5, 65.67, 58.33, 54.17, 51.67, 51.17, 47.17]	[nan, 78.83, 77.17, 69.61, 66.75, 63.13, 59.7, 57.81, 55.64]	[86.17, 77.17, 73.94, 68.62, 65.07, 61.64, 58.55, 56.98, 54.7]	[0.24, 0.24, 0.08, 0.0, 0.09, 0.04, 0.03, 0.03, 0.02]	dtd_baseline_lora_fscit	dtd	./data	baseline_lora	64	10	0.01	Cosine	[60, 70]	40	0.0005	0.9	1.0	128	65536	0.999	0.07	0.0	2	google/vit-base-patch16-224-in21k	vit-b16	1	10	FSCIT	False	100	15	8	2	3	LoRA	5	0	100	[0, 1, 2]	0	1	-1	1.0	False	False	True	False	5	5	47	9	checkpoint/dtd_baseline_lora_fscit	[224, 224]
