base_last	incremental_last	all_last	all_std_last	base	incremental	all	all_std	exp_name	dataset	dataroot	result_key	batch_size_base	epochs_base	lr_base	schedule	milestones	step	decay	momentum	ce_loss_factor	moco_dim	moco_k	moco_m	moco_t	moco_loss_factor	num_views	hf_model_checkpoint	encoder	num_mlp	shot	fsl_setup	incft	test_batch_size	eval_freq	num_workers	seed	num_seeds	pet_cls	rank	pet_tuning_start_epoch	encoder_ft_start_layer	adapt_blocks	encoder_ft_start_epoch	encoder_lr_factor	limited_base_class	limited_base_samples	pet_on_teacher	update_base_classifier_with_prototypes	start_training_with_prototypes	add_bias_in_classifier	base_class	way	num_classes	sessions	save_path	size_crops
82.13	74.84	75.57	0.0	[95.87, 92.73, 87.93, 87.63, 85.83, 84.83, 84.03, 83.33, 82.53, 82.13]	[nan, 85.23, 83.32, 81.16, 78.68, 76.14, 75.94, 75.48, 75.95, 74.84]	[95.87, 88.98, 84.86, 82.77, 80.11, 77.59, 77.1, 76.46, 76.68, 75.57]	[0.05, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0]	cifar100_baseline_lora_fscit	cifar100	./data	baseline_lora	64	10	0.001	Cosine	[60, 70]	40	0.0005	0.9	1.0	128	65536	0.999	0.07	0.0	2	google/vit-base-patch16-224-in21k	vit-b16	1	10	FSCIT	False	100	15	8	2	3	LoRA	5	0	100	[0, 1, 2]	0	1	-1	1.0	False	False	True	False	10	10	100	10	checkpoint/cifar100_baseline_lora_fscit	[224, 224]
82.47	75.04	75.78	0.05	[96.37, 93.27, 88.3, 88.0, 86.07, 85.23, 84.3, 83.63, 82.83, 82.47]	[nan, 86.17, 83.77, 81.75, 79.0, 76.44, 76.17, 75.68, 76.14, 75.04]	[96.37, 89.72, 85.28, 83.31, 80.41, 77.91, 77.33, 76.67, 76.89, 75.78]	[0.05, 0.08, 0.14, 0.15, 0.1, 0.05, 0.04, 0.03, 0.03, 0.05]	cifar100_baseline_lora_fscit	cifar100	./data	baseline_lora	64	10	0.1	Cosine	[60, 70]	40	0.0005	0.9	1.0	128	65536	0.999	0.07	0.0	2	google/vit-base-patch16-224-in21k	vit-b16	1	10	FSCIT	False	100	15	8	2	3	LoRA	5	0	100	[0, 1, 2]	0	1	-1	1.0	False	False	True	False	10	10	100	10	checkpoint/cifar100_baseline_lora_fscit	[224, 224]
82.67	75.05	75.81	0.01	[96.37, 93.1, 88.43, 88.13, 86.23, 85.33, 84.43, 83.83, 83.07, 82.67]	[nan, 86.17, 83.82, 81.81, 79.08, 76.47, 76.19, 75.7, 76.15, 75.05]	[96.37, 89.63, 85.36, 83.39, 80.51, 77.94, 77.37, 76.71, 76.92, 75.81]	[0.12, 0.06, 0.1, 0.05, 0.02, 0.03, 0.02, 0.01, 0.0, 0.01]	cifar100_baseline_lora_fscit	cifar100	./data	baseline_lora	64	10	0.01	Cosine	[60, 70]	40	0.0005	0.9	1.0	128	65536	0.999	0.07	0.0	2	google/vit-base-patch16-224-in21k	vit-b16	1	10	FSCIT	False	100	15	8	2	3	LoRA	5	0	100	[0, 1, 2]	0	1	-1	1.0	False	False	True	False	10	10	100	10	checkpoint/cifar100_baseline_lora_fscit	[224, 224]
